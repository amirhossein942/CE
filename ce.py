# -*- coding: utf-8 -*-
"""CE

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/16GF5Um0iIzuxRHP1xcF2JAsS3ybfP5Ks

#Topic Modeling with Latent Semantic Analysis

##Importing all necessary libraries 
For this project, there are many libraries needed to be imported, and they can all be viewed below.
"""

from google.colab import files 
import seaborn as sns
import numpy as np 
import pandas as pd
pd.set_option('display.width', 1000)
import os
import re
import nltk
from nltk.tokenize import word_tokenize 
import pandas as pd
from nltk.stem.porter import PorterStemmer
nltk.download('stopwords')
nltk.download('punkt')  
from nltk.corpus import stopwords 
import matplotlib.pyplot as plt
from sklearn.decomposition import TruncatedSVD
from sklearn.metrics import confusion_matrix
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.pipeline import make_pipeline
from sklearn.preprocessing import Normalizer
from sklearn.linear_model import LogisticRegression
from sklearn import metrics

"""##Importing the Data

In this section, the data is imported. It consists of facebook comments that have been divided into three categories: "Overtly Aggressive", "Covertly Aggressive", and "Non Aggressive". Because the dataset originally lacked any names for each column, specific column names are added to the dataset by using "names=['number', 'text', 'category']" in order to identify each column with ease.
"""

train=pd.read_csv('https://raw.githubusercontent.com/amirhossein942/CE/main/agr_en_train.csv', names=['number', 'text', 'category'])    
train

"""##Preparing the Data

In order to split the data into two categories of 1s and 0s, only the "NAG" and "OAG" categories, which consists of non aggressive and overtly aggressive respectively, have been kept. The non aggressive comments are all given the value 1 while the overtly aggressive comments have all been given the value 0. At the end, each column is then merged together.
"""

train1 = train.loc[(train['category']=='NAG')]
train1['category']=1

train2 = train.loc[(train['category']=='OAG')]
train2['category']=0

train=  pd.concat([train1, train2])
train

"""##Data Preprocessing 
After importing the data, preprocessing is an important step to take. Firstly, the urls and links to websites are removed and replaced by a white space by creating search patterns using RegEx. Secondly, a function is defined in which any unicode and emoji character is removed by encoding the unicode.Then, the letters are turned into lowercase letters, the text is also tokenized, English stop words are removed from the text, and stemming is then applied. The function returns the transformed text afterwards. Lastly, all remaining special characters, including punctuation characters and numbers, are removed from the text and replaced with a white space by commanding RexEg to remove everything except letters from a-z and A-Z which constitutes the English alphabet. However, since the letters have been turned into lowercase letters, then all letters are whithin the ranges of a-z. 
"""

train['text']= train['text'].str.replace(r'http\S+|www.\S+', '') 

def prep(text):
  text=text.encode("ascii", "ignore")
  text = text.decode()
  text=text.lower()
  token=word_tokenize(text) 
  stp_wrd = set(stopwords.words('english'))
  text = " ".join([i for i in token if i not in stp_wrd])
  text = [PorterStemmer().stem(word) for word in text.split(" ")]
  return " ".join(text)
train['text']= train['text'].str.replace(r"[^a-zA-Z]+"," ")

train['text']=train['text'].apply(lambda x: prep(x))
train['text']

train

"""##Making a Normalized Term Document Matrix using Td-idf
In order to apply the Latent Semantic Analysis, the data must be vectorized and a Term Document Matrix must be created
"""

Target=train['category']

vectorizer = TfidfVectorizer(
    max_df=0.5,
    max_features=10000,
    min_df=2,
    stop_words='english',
    use_idf=True
    )
X = vectorizer.fit_transform(train['text'])

"""##Latent Semantic Analysis
Here we use the LSA technic after making the Term Document Matrix
"""

n_components = 2

svd = TruncatedSVD(n_components=n_components) 
normalizer = Normalizer(copy=False)
lsa = make_pipeline(svd,normalizer)
X_lsa = lsa.fit_transform(X)

X_lsa.shape

clf=LogisticRegression()
clf.fit(X=X_lsa, y=Target)

metrics.accuracy_score(y_true=Target, y_pred=clf.predict(X_lsa))

metrics.confusion_matrix(y_true=Target, y_pred=clf.predict(X_lsa))

cf=confusion_matrix(y_true=Target, y_pred=clf.predict(X_lsa))
plt.figure(figsize=(10,8))
sns.heatmap(cf, annot=True, cmap='autumn')
plt.xlabel('predicted')
plt.ylabel('actual')
plt.show()

"""###With the LSA technic, an accuracy score of 65% is found. This may suggest that the usage of other technics might be better to make more accurate predictions. Additionally, the provided confusion matrix shows how the prediction for overtly aggressive text has been the least accurate. However, the prediction for non aggressive text is more reasonable than its counter part.

#LSA Analysis
"""

X_lsa_df=pd.DataFrame(X_lsa) 
title_font = {
    'family': 'serif',
    'color':  'xkcd:midnight', 
    'weight': 'bold',
    'size': 15,
}
label_font = {
    'family': 'serif',
    'color':  'xkcd:black',
    'weight': 'normal',
    'size': 14,
}
plt.figure(figsize=[15,6.5], dpi=110)
plt.title(
    "Weight of Terms in Each Component", 
   fontdict=title_font,
    loc='left',
    pad=10
)
plt.scatter(x=X_lsa_df[0],y=X_lsa_df[1],c=Target)
plt.xlabel(
    "First Component",
    fontdict=label_font,
    labelpad=8
)

plt.ylabel(
    "Second Component",
    fontdict=label_font,
    labelpad=8
)

plt.show()

"""###The figure above demonstrates the weights associated to every term found in each component. The x-axis represents the first component, and the y-axis represents the second component.

Now that the components have been created and divided, it's best to see the list of words in each one. Below, the top 15 words from each component are shown by going throught each "svd.component_" and printing the desired number of terms. Looking at the first topic, an assumption about the type of words can be given. There are certain words about nationality and there is one word about religion, army and killing. This leads the topic to be more politically related. The second component on the other hand seems to be more related to economics as there are words about the market, stocks and banks. However, there is further analysis to be done since the weights of the words are also important to see.
"""

terms = vectorizer.get_feature_names() 

for i, comp in enumerate(svd.components_):
    terms_comp = zip(terms, comp)
    sorted_terms = sorted(terms_comp, key= lambda x:x[1], reverse=True)[:15]
    print("Topic "+str(i+1)+": ")
    for t in sorted_terms:
        print(t[0])
    print(" ")

topics=[f"topic_{i}" for i in range(1,n_components+1)] 
topics

"""Below, each component has been divided into a separate DataFrame in which the terms and their corresponding weights can be observed. By following the previous analysis, the top 15 words and their weights are shown in a descending order. Additionally, the top 15 words and their weights are illustrated through bar plots. """

first_topic_words=pd.DataFrame(
    [svd.components_[0]],
    index=[topics[0]],
    columns=terms
    ).T 
ordered_topic_words1=first_topic_words['topic_1'].sort_values(ascending=False).head(15)
ordered_topic_words1

ax=ordered_topic_words1 .plot(
kind='bar',
figsize=(15, 6.5),
color=['xkcd:indigo blue', 'xkcd:royal purple', 'xkcd:sunflower yellow', 'xkcd:butterscotch', 'xkcd:clear blue', 'xkcd:purplish red', 'xkcd:celery', 'xkcd:liliac', 'xkcd:dust', 'xkcd:chartreuse', 'xkcd:scarlet', 'xkcd:warm grey', 'xkcd:canary yellow', 'xkcd:silver', 'xkcd:gold'],
)


parameters = {'axes.labelsize': 25,
          'axes.titlesize': 35}
plt.rcParams.update(parameters)

ax.set_title(
    "Top 15 Words Related to the First Concept", 
   fontdict=title_font,
    loc='left',
    pad=10,
)

ax.set_xlabel(
    "Words",
    fontdict=label_font,
    labelpad=13
)

ax.set_ylabel(
    "Weight",
    fontdict=label_font,
    labelpad=8
)

plt.show()

second_topic_words=pd.DataFrame(
    [svd.components_[1]],
    index=[topics[1]],
    columns=terms
    ).T 
ordered_topic_words2=second_topic_words['topic_2'].sort_values(ascending=False).head(15)
ordered_topic_words2

ax=ordered_topic_words2.plot(
kind='bar',
figsize=(15, 6.5),
color=['xkcd:indigo blue', 'xkcd:royal purple', 'xkcd:sunflower yellow', 'xkcd:butterscotch', 'xkcd:clear blue', 'xkcd:purplish red', 'xkcd:celery', 'xkcd:liliac', 'xkcd:dust', 'xkcd:chartreuse', 'xkcd:scarlet', 'xkcd:warm grey', 'xkcd:canary yellow', 'xkcd:silver', 'xkcd:gold'],
)

parameters = {'axes.labelsize': 25,
          'axes.titlesize': 35}
plt.rcParams.update(parameters)

ax.set_title(
    "Top 15 Words Related to the Second Component", 
   fontdict=title_font,
    loc='left',
    pad=10,
)

ax.set_xlabel(
    "Words",
    fontdict=label_font,
    labelpad=8
)

ax.set_ylabel(
    "Weight",
    fontdict=label_font,
    labelpad=8
)

plt.show()

"""Lastly, the related texts for each of the components have also been shown. This is done by putting the texts as the index and putting each component as a column and showing the weight of every text within each component. For the sake of simplicity, only the first five text are shown in a descending order of the weights. Interestingly, the texts of the first component all seem to be complete texts and sentences while the texts of the second component are only the word "good" with the exception of one that has an additional "dt". """

text_topics=pd.DataFrame(
    X_lsa,
    index=train['text'], 
    columns=topics
)

text_topics['topic_1'].sort_values(ascending=False).head(5)

text_topics['topic_2'].sort_values(ascending=False).head(5)

"""In summary, the LSA technic was used and analyzed in this project. The LSA technic gave an accuracy score of 65% which can be more accurate by using other topic modeling technics. One way to see the issue is by looking at the confusion matrix where the prediction of overtly aggressive texts has been the least accurate. The top 15 words of each component including their weights were shown. There seems to be a division of category between political and economical related terms. Finally, the texts of each topic were shown where there was a lack of actual sentences and texts within the second topic. """